하둡을 사용해서 병렬처리?? -> 빅데이터 분석 문제들에 대해서 맵리듀스 알고리즘을 자바언어로 구현하고 실행한다.

Scale-out 아주 많은 값싼 서버들을 이용함
Scale-up 적은 수의 값비싼 서버들을 이용함

데이터 중심 어플리케이션 분야에서는 아주 많은 값싼 서버들을 많이 이용하는 것을 선호함
고가의 서버들은 가격에 관점에서는 선형으로 성능이 증가하지 않음 -> 두 배의 성능의 프로세서 한 개를 가진 컴퓨터의 가격이 일반적인 프로세서 한 개를 가진 컴퓨터 가격의 두 배보다 훨씬 더 비쌈

맵리듀스 프레임워크는 많은 컴퓨터를 묶어서 처리한다. 각각의 머신에서 동시에 mapping 한다(각각의 머신 안에서는 한줄씩 map함수가 실행되어서 직렬처럼 보이지만)-> 병렬처리

데이터 중심 프로세싱

- 한대의 컴퓨터의 능력으로 처리가 어려움
- 근본적으로 수 십대, 수백대 혹은 수 천대의 컴퓨터를 묶어서 처리해야함
- 맵리듀스 프레임 워크가 하는 것이 바로 이것
  맵리듀스는 빅데이터를 이용한 효율적인 계산이 가능한 첫 번째 프로그래밍 모델
  구글의 맵리듀스 또는 오픈소스인 하둡은 맵리듀스 프레임워크의 우수한 구현의 형태임

MapReduce Programming Model -> 함수형 프로그래밍 언어의 형태
유저는 아래 3가지 함수를 구현해서 제공해야함
Main 함수
Map 함수 : (key1, val1) -> [(key2, val2)]
Reduce 함수 : (key2, [val2]) -> [(key3, val3)]

맵리듀스 프레임워크에서는 각각의 레크드 또는 튜플은 키-밸류 쌍으로 표현된다.

맵리듀스 페이즈는 3단계로 수행되는데,

아래에서 자세히

1. 맵 페이즈는 제일 먼저 수행되며 데이터의 여러 파티션에 병렬 분산으로 호출되어 수행된다.
   각 머신마다 수행된 mapper는 맵 함수가 입력 데이터릐 한 줄 마다 맵 함수를 호출한다.
2. 셔플링 페이즈는 모든 머신에서 맵 페이즈가 다 끝나면 시작된다.
   맵 페이즈에서 각각의 머신으로 보내진 key, value 쌍을 key 를 이용해서 sorting 한 후 에 각각의 key마다 같은 key를 가진 key, value 쌍을 모아서 밸류리스트를 만든 다음에 key, value-list 형태로 key에 따라서 여러 머신에 분산해서 보낸다.
3. 리듀스페이즈 에서는 셔플링 페이즈가 다 끝나면 각 머신마다 리듀스 페이즈가 시작되는데 각각의 머신에서는 셔플링 페이즈에서 해당 머신으로 보내진 각각의 쌍 마다 리듀스 함수가 호출되며 하나의 리듀스 함수가 끝나면 다음 쌍에 리듀스 함수가 호출된다.
   출력이 있다면 key, value 쌍의 형태로 출력된다.

각각의 document에서 mapper로 key value를 만든 다음에 (중복된 key) 특정 조건에 따라서 각각을 머신으로 보내고 (반드시 동일한 키는 동일한 머신으로 ) sorting 한다. 이후에 셔플링 페이즈에서 key값에 따라서 value 를 리스트로 만든다 (각각의 머신마다 발생) 리듀스 페이즈에서 동일한 key 의 value list를 값으로 만든다.

하둡 분산 파일 시스템(Hadoop Distributed File System - HDFS)

- 빅 데이터 파일을 여러 대의 컴퓨터에 나누어서 저장함
- 각 파일은 여러 개의 순차적인 블록으로 저장함
- 하나의 파일의 각각의 블록은 폴트 톨러런스(fault tolerance)를 위해서 여러 개로 복사되어 여러 머신의 여기저기 저장됨
  폴트 톨러런스는 시스템을 구성하는 부품의 일부에서 결함 또는 고장이 발생하여도 정상적 혹은 부분적으로 기능을 수행할 수 있는 것을 말함

#### Combine 함수

Map 함수의 결과 크기를 줄여준다. -> 셔플링 비용을 줄여준다.
각각의 머신에서 map의 결과를 combine 해서 key 에 따른 value값을 미리 합쳐주고 셔플링으로 보낸다.

---

#### Map함수

org.apache.hadoop.mapreduce라는 패키지에 있는 Mapper 클래스를 상속받아서 맵 메소드를 수정한다.

```
public static class TokenizerMapper
			extends Mapper<Object,Text,Text,IntWritable> {

		// variable declairations
		private final static IntWritable one = new IntWritable(1);
		private Text word = new Text();

		// map function (Context -> fixed parameter)
		public void map(Object key, Text value, Context context)
				throws IOException, InterruptedException {

			// value.toString() : get a line
			StringTokenizer itr = new StringTokenizer(value.toString());
			while ( itr.hasMoreTokens() ) {
				word.set(itr.nextToken());

				// emit a key-value pair
				context.write(word,one);
			}
		}
	}

```

여기서 Object key에는 입력 텍스트 파일에서 맨 앞 문자를 기준으로 맵 함수가 호출된 해당 라인의 첫 번째 문자까지의 오프셋(매 라인마다 map이 호출된다고 했는데 그런 호출라인의 정보)
Text value에는 텍스트 해당 라인 전체가 들어있다.

#### Reduce함수

org.apache.hadoop.mapreduce라는 패키지에 있는 Reducer 클래스를 상속받아서 reduce 메소드를 수정한다.

```
public static class IntSumReducer
			extends Reducer<Text,IntWritable,Text,IntWritable> {

		// variables
		private IntWritable result = new IntWritable();

		// key : a disticnt word
		// values :  Iterable type (data list)
		public void reduce(Text key, Iterable<IntWritable> values, Context context)
				throws IOException, InterruptedException {

			int sum = 0;
			for ( IntWritable val : values ) {
				sum += val.get();
			}
			result.set(sum);
			context.write(key,result);
		}
	}

```

셔플링 페이즈의 출력을 입력으로 받는데 key, value-list 형태 -> Text key , Iterable<IntWritable> values
value-list는 맵 함수의 출력에서 key를 갖는 key,value 쌍들의 value들의 리스트

---

맵 리듀스의 3페이즈(map, shuffle, reduce)

1. 각각의 머신에서 documents를 mapping의 map을 사용해서 한줄씩 map함수를 실행하면 각각의 머신에서 key와 value를 가진 key,value쌍이 나온다. 각각의 머신에서 따로 처리 하기때문에 병렬처리 이다.
   그리고 처리된결과를 mapreduce framework가 머신에 보내준다.(같은 키는 같은 머신으로 보내주자) 여기까지가 map phase
2. 셔플링페이즈에서는 map에서 나온 key,value 를 sorting하고 각 key마다 그 key 를 가진 value들을 모아서 value-list를 만든다. 즉, (key, value-list) 모양을 만든다.
3. 이렇게 만들어진 key,value-list 각각마다 reduce함수가 하나씩 호출된다. 그 결과로 key, value 쌍이 만들어진다.

만약 combine이 있다면 map의 output을 바로 shuffling phase로 보내는 것이 아니라 중복 key에 따른 value를 합친 결과 key, value를 보낸다.

정리 : map의 결과 key,value 쌍의 리스트가 만들어짐, shuffling의 결과 key,value-list 쌍의 리스트가 만들어짐, reduce 는 key,value-list 쌍의 리스트를 입력으로 받고 key,value로 만들어준다.

---

### 하둡의 설치와 사용법 (window 기준)

VMware 최신 버전 설치 -> Ubuntu 최신 버전 설치 -> Hadoop 최신 버전 설치

VMware 설치

```
https://www.vmware.com/products/workstation-player.html
```

VMware에 Ubuntu설치

```
http://www.ubuntu.com/download/ubuntu/download
```

iso파일이 설치가 되면 vmware에서 create New Machine

Full name은 자유롭게 ex) ubuntu
User name : hadoop

ubuntu에서 terminal 접속

```
$ wget http://kdd.snu.ac.kr/~kddlab/Project.tar.gz
$ tar zxf Project.tar.gz
$ sudo chown -R hadoop:hadoop Project
$ cd Project
$ sudo mv hadoop-3.2.2 /usr/local/hadoop
$ sudo apt update
$ sudo apt install ssh openjdk-8-jdk ant -y
$ ./set_hadoop_env.sh
$ source ~/.bashrc

```

hadoop 계정에서 ssh key generation(리눅스에서 하둡을 돌리려면 pwd가 필요한데 안치고도 돌릴수 있도록 ssh키 생성)

```
$ ssh-keygen -t rsa -P ""
```

저장할 파일을 물어보면 default로 enter
생성된 id_rsa.pub을 authorized_keys에 넣는다.
직금은 하나지만 각 서버에 있는 모든 파일을 하나의 authorized_keys에 모은다.

```
$ cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys
```

제대로 생성되었는지 확인

```
$ ssh localhost
```

질문이 뜨면 yes를 입력하고 그 다음 비밀번호를 물어보지 않고 prompt가 뜨면 성공

만약 제대로 등록하였는데 계속 비밀번호를 입력하라고 한다면

```
$ restorecon -R -v /root/.ssh
```

Path를 지정하기 위해

```
$ cd /home/hadoop
$ source .bashrc
```

Name node format(Disk format과 같은 개념)

```
$ hadoop namenode -format
```

Dfs daemon start

```
$ start-dfs.sh
```

MapReduce daemon start(standalone 모드(컴퓨터한대)에서는 불필요)

```
$ start-mapred.sh
```

확인

```
$ jps
```

SecNameNode, ondaryNameNode, DateNode, TaskTracker(Standalone에서 불필요), JobTracker(Standalone에서 불필요)

이렇게 생성하면 리눅스 디렉토리와 hadoop 디렉토리가 있다.
src/ Driver.java(맵리듀스 코드 컴파일을 위한 파일)

hadoop(HDFS)에는 디렉토리를 만들어 줄 것 이다.

만든 코드를 실행하려면 맵리듀스 코드 컴파일을 위해서 만들어 놓은 Driver.java 파일을 수정해야 한다.

```
pgd.add("사용명(보통 자바파일명)", 자바파일명.class, "comment");
```

**Driver.java 파일이 바뀌면 반드시 ant를 다시 수행해야함**

```
$ cd ~/Project
$ ant
- 유닉스의 make 같은것으로 java개발환경에서의 표준 빌드 도구
- 자바 파일을 컴파일하는 javac도 있지만, ant 는 여러 dependency를 고려하여 소스피일을 컴파일
- src 디렉토리에 있는 것을 다 모아서 컴파일 한 후에 jar를 만든다.
- build.xml에 정의한 대로 수행된다.
```

그리고 위에서 설정한 자바 파일과

```
hadoop jar buildxml에 정의한이름.jar 사용명(보통 자바파일명) 하둡의input디렉토리 하둡의output디렉토리
```

일치 시켜줘야함

데이터 테스터를 HDFS에 넣어야 한다.

```
$ cd /home/hadoop/Project
$ hdfs dfs -mkdir wordcount_test (만약 디렉토리가 없다고 나오면 오류md 참고)
- 하둡의 HDFS에 word_test 디렉토리를 생성
$ hdfs dfs -put data/wordcount-data.txt wordcount_test
- Linux의 data 디렉토리에 있는 wordcount-data.txt 파일을 하둡의 HDFS의 wordcount_test디렉토리로 보냄
```

**반드시 맵리듀스 프로그램이 결과를 저장할 디렉토리를 삭제한 후 프로그램을 실행해야함**

```
$ hdfs dfs -rm -r word_test_out (하둡의 output디렉토리)
```

Wordcount MapReduce 알고리즘 코드 실행

```
$ cd /home/hadoop/Project
$ hadoop jar ssafy.jar wordcount wordcount_test wordcount_test_out
- Driver.java에 표시한대로 wordcount를 써서 Wordcount 맵 리듀스 코드를 수행
- Wordcount_test 디렉토리에 들어있는 모든 파일을 맵 함수의 입력으로 사용함
```

하둡의 실행방법

```
$ hadoop jar jarfile programname inputDirectory outputDirectory
```

결과확인
reducer개수에 따라서 출력파일이 달라진다. 2개인경우

```
$ hdfs dfs -cat wordcount_test_out/part-r-00000 | more
- 0번 reducer가 출력한 파일의 내용을 보여줌
- more 을 해야 안짤리고 계속 볼수있다.

$ hdfs dfs -cat wordcount_test_out/part-r-00001 | more
- reducer의 개수 -1개 까지 넣어서 보면된다.
```

---

#### 하둡의 맵리듀스의 map함수, reduce함수, combine함수 등에서 입출력의 파라미터로 쓰는 클래스와 자바 타입

- Text : String
- IntWritable : Int
- LongWritable : long
- FloatWritable : float
- DoubleWritable : double

만약 새로운 타입의 클래스를 정의해서 입출력에 사용하고 싶다면 필요한 여러 함수도 함께 정의를 해주어야한다.

---

#### Wordcount.java 를 살펴보자

```
package ssafy; //반드시 build.xml에서 사용한걸 사용

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class Wordcount {
	/*
	Object, Text : input key-value pair type (always same (to get a line of input file))
	Text, IntWritable : output key-value pair type
	*/
	public static class TokenizerMapper
			extends Mapper<Object,Text,Text,IntWritable> {	//Mapper를 상속받는다. map함수의 입출력 : 편의상 Object로 넣었고 key, Text value 형태로 입력 된다.  출력은 Text key, IntWritable value 이다.

		// variable declairations
		private final static IntWritable one = new IntWritable(1); // 변수이름이 one 인 intwritable을 생성하고 1의 값을 고정으로 넣는다.
		private Text word = new Text();

		// map function (Context -> fixed parameter)
		public void map(Object key, Text value, Context context)
				throws IOException, InterruptedException {

			// value.toString() : get a line
			StringTokenizer itr = new StringTokenizer(value.toString()); // value는 String이 아니라서 toString해줘야함. String 을 Tokenizer화 해준다.
			while ( itr.hasMoreTokens() ) {// 더이상 토큰이 없을때 까지
				word.set(itr.nextToken());// Text타입으로 만든 word에 값을 set 해준다.

				// emit a key-value pair
				context.write(word,one);// 각각의 Text와 1을 넣어준다. (key, value)
			}
		}
	}

	/*
	Text, IntWritable : input key type and the value type of input value list
	Text, IntWritable : output key-value pair type
	*/
	public static class IntSumReducer
			extends Reducer<Text,IntWritable,Text,IntWritable> {// reducer를 상속해서 만든다. input은 Text, IntWritable ouput도 동일

		// variables
		private IntWritable result = new IntWritable();

		// key : a disticnt word
		// values :  Iterable type (data list)
		public void reduce(Text key, Iterable<IntWritable> values, Context context) // intwritable타입의 리스트
				throws IOException, InterruptedException {

			int sum = 0;
			for ( IntWritable val : values ) {
				sum += val.get();
			}
			result.set(sum);
			context.write(key,result);
		}
	}


	/* Main function */
	public static void main(String[] args) throws Exception {
		Configuration conf = new Configuration();
		String[] otherArgs = new GenericOptionsParser(conf,args).getRemainingArgs();
		if ( otherArgs.length != 2 ) {
			System.err.println("Usage: <in> <out>");
			System.exit(2);
		}
		Job job = new Job(conf,"word count"); //job 작성, 따옴표안은 설명을 쓰면됨
		job.setJarByClass(Wordcount.class); //job을 수행할 class 선언, 파일명.class, 대소문자주의

		// let hadoop know my map and reduce classes
		job.setMapperClass(TokenizerMapper.class); //Map class 선언, 위에서 작성한 class명
		//job.setCombinerClass(IntSumReducer.class); //Combiner class 선언, reducer함수의 클래스를 넣는다.
		job.setReducerClass(IntSumReducer.class); //Reduce class 선언

		job.setOutputKeyClass(Text.class); //Output key type 선언, reduce의 key type
		job.setOutputValueClass(IntWritable.class); //Output key value 선언, reduce의 value type

		//job.setMapOutputKeyClass(Text.class); //Map은 Output key type가 다르다면 선언
		//job.setMapOutputKeyClass(IntWritable.class); //Map은 Output value type가 다르다면 선언

		// set number of reduces
		job.setNumReduceTasks(2); //동시에 수행되는 reducer 개수

		// set input and output directories
		FileInputFormat.addInputPath(job,new Path(otherArgs[0])); //입력 데이터가 있는 path
		FileOutputFormat.setOutputPath(job,new Path(otherArgs[1])); //결과를 출력할 path
		System.exit(job.waitForCompletion(true) ? 0 : 1 ); //실행
	}
}
```

**Driver.java 파일뿐만 아니라 Wordcount.java가 바뀌면 반드시 ant를 다시 수행해야함**
결과를 볼때는 항상 **outputDirectory를 삭제해야한다**
결과 파일들은 part-r-00000 .... 으로 나온다 (part-r-(reducer개수-1))

Main 함수에서 Mapper나 Reducer에 값을 Broadcast하려면

```
Configuration config = job.getConfiguration();
config.set("name","Shim"); //String 은 set , name이라는 symbol의 값은 shim이다.
config.setInt("one",1);
config.setFloat("point_five",(float)0.5);
```

이렇게 메인에서 지정하고

reducer나 mapper에서

```
protected void setup(Context context) throws IOExeption, InterruptedException { // setup은 mapper나 reduce에서 돌기전에 가장먼저 하는것
	Configuration config = context.getConfiguration();
	String name = config.get("name","kim"); // main에서 name이라는 symbol에 값이 broadcast되지 않았다면 default로 kim을 지정
	int point = config.getInt("one",1);
	float rate = config.getFloat("point_five",(float)0.5);
}
```

---

Day2
Partitioner Class, Inverted Index, Matrix Addition

#### Partitioner Class

보통 Map함수의 출력인 key,value 쌍이 key에 의해서 어느 Reducer로 갈지가 정해지는데 (하둡의 기본타입(text, IntWritable, LongWritable .. )은 Hash 함수가 Default로 제공되고 있어서 key에 대한 해시 값에 따라 어느 reducer로 갈지 정한다.) 이러한 클래스를 재정의로 사용자가 원하는대로 원하는 reducer(머신)으로 보내보자

만약 2개의 머신을 사용하는데
1머신에 홀수
2머신에 짝수
이런식으로 보내버리면 나중에 다시 merge해서 sort해야하는 불편함이 존재한다.

Partitioner을 상속받아서 MyPartiotioner라는 클래스를 생성하고
getPartition을 Override해서 kye의 첫번째 문자가 a보다 작은경우와 큰경우를 나눠서 머신으로 return한다.
여기서 numPartions는 educer의 개수를 몇개 세팅했냐를 알려줌

```
public static class MyPartitioner extends Partitioner<Text,IntWritable> {
       @Override
       public int getPartition(Text key, IntWritable value, int numPartitions){
           String nbOccurences = key.toString();
           if (nbOccurences.charAt(0) < 'a'){
               return 0;	//0번 reducer
           }
           else return 1;	//1번 reducer
       }
   }
```

이러한 MyPartition의 재정의로 map함수의 출력이 달라진다.(원래는 그냥 key 별로 sorting한 값이 나가는데 이번에는 key의 첫번째 문자가 a보다 작으면 1번 reducer로 a보다 크면 2번 reducer로 sorting해서 나간다.)

main함수에

```
job.setPartitionerClass(MyPartitioner.class);
```

를 추가해준다.

그리고 Partitioner클래스를 추가해준다.

```
import org.apache.hadoop.mapreduce.Partitioner;
```

키와 value의 타입이 달라지지는 않는다.
Wordcountsort의 전체 코드

```
package ssafy;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.mapreduce.Partitioner;

public class Wordcountsort {
   /*
   Object, Text : input key-value pair type (always same (to get a line of input file))
   Text, IntWritable : output key-value pair type
   */
   public static class TokenizerMapper
           extends Mapper<Object,Text,Text,IntWritable> {

       // variable declairations
       private final static IntWritable one = new IntWritable(1);
       private Text word = new Text();

       // map function (Context -> fixed parameter)
       public void map(Object key, Text value, Context context)
               throws IOException, InterruptedException {

           // value.toString() : get a line
           StringTokenizer itr = new StringTokenizer(value.toString());
           while ( itr.hasMoreTokens() ) {
               word.set(itr.nextToken());

               // emit a key-value pair
               context.write(word,one);
           }
       }
   }

   /*
   Text, IntWritable : input key type and the value type of input value list
   Text, IntWritable : output key-value pair type
   */
   public static class IntSumReducer
           extends Reducer<Text,IntWritable,Text,IntWritable> {

       // variables
       private IntWritable result = new IntWritable();

       // key : a disticnt word
       // values :  Iterable type (data list)
       public void reduce(Text key, Iterable<IntWritable> values, Context context)
               throws IOException, InterruptedException {

           int sum = 0;
           for ( IntWritable val : values ) {
               sum += val.get();
           }
           result.set(sum);
           context.write(key,result);
       }
   }

   public static class MyPartitioner extends Partitioner<Text,IntWritable> {
       @Override
       public int getPartition(Text key, IntWritable value, int numPartitions){
           String nbOccurences = key.toString();
           if (nbOccurences.charAt(0) < 'a'){
               return 0;
           }
           else return 1;
       }
   }


   /* Main function */
   public static void main(String[] args) throws Exception {
       Configuration conf = new Configuration();
       String[] otherArgs = new GenericOptionsParser(conf,args).getRemainingArgs();
       if ( otherArgs.length != 2 ) {
           System.err.println("Usage: <in> <out>");
           System.exit(2);
       }
       Job job = new Job(conf,"Wordcountsort");
       job.setJarByClass(Wordcountsort.class);

       job.setPartitionerClass(MyPartitioner.class);

       // let hadoop know my map and reduce classes
       job.setMapperClass(TokenizerMapper.class);
       // job.setCombinerClass(IntSumReducer.class);
       job.setReducerClass(IntSumReducer.class);
```

#### Inverted Index 생성

```
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;	// 현재파일의 이름을 알기위해서 사용
import org.apache.hadoop.fs.FileSystem;
```

##### inverted index란??

지정한 단어가 각각의 doc 어디서 나타나는지 그 위치를 리스트로 가지고 있는 것이 inverted list
ex)
Doc1 : IMF Financial Economics Crisis
Doc2 : IMF Financial Crisis
IMF -> Doc1:1, Doc2:1
.....
검색엔진에서 A B 검색하면 각각의 invertlist를 intersection 해서 찾아낸다. -> A ~:~ ,~:~ B ~:~, ~:~ 이런식으로 어디에 존재하는지 찾는다. 그리고 두개의 단어를 intersectino함 그러면 두 단어를 모두 포함한 문서를 찾을 수 있다
각각의 doc마다 map함수가 처리함, 머신이 따로따로 처리해서 병렬처리 -> 결과 : key가 해당파일의 몇번째 offset에 존재하는지 출력 key(text),value(text) 형태
같은 key마다 shuffling 을 통해서 value-list로 만들어진다. -> 결과 : key를 모두 합쳐서 해당 key가 어느 파일의 몇번째 offset에 존재하는지 출력 key(text),value(list(text))
이 전에는 shuffling을 통해서 만들어진 value-list를 reduce를 통해 합쳐진 InterWritable 형태였지만
reduce함수가 key,value-list 쌍마다 call되면 각각의 output fileformat으로 바꿔서 출력만 그대로 해준다.
즉, reduce와 shuffling의 출력 형식은 같고 reduce가 fileformat으로 바꿔주는것 -> reducer 결과 : suffling output과 동일

##### 알아야 할것!

splitting에서 넘어온 데이터를 mapper의 map함수를 사용해서 자를껀데 이 단어들의 offset을 알아야한다!
splitting에서 넘어온 데이터들의 현재 file이름을 알아야 한다!

```
public static class TokenizerMapper
           extends Mapper<Object,Text,Text,Text> {

       private String filename;

       protected void setup(Context context) throws IOException, InterruptedException {
           filename = ((FileSplit)context.getInputSplit()).getPath().getName();
       }

       // variable declairations
       private Text word = new Text();
       private Text resVal = new Text();

       // map function (Context -> fixed parameter)
       public void map(Object key, Text value, Context context)
               throws IOException, InterruptedException {
           Long position = ((LongWritable)key).get();
           // value.toString() : get a line
           StringTokenizer itr = new StringTokenizer(value.toString(), " ",true);
           while ( itr.hasMoreTokens() ) {
//               Text word = new Text();
//               Text resVal = new Text();
               String token = itr.nextToken();
               word.set(token.trim());
               if(!token.equals(" ")){
                   resVal.set(filename+":"+position);
                   context.write(word,resVal);
               }

               position += token.length();

           }
       }
   }


```

**map함수의 key에는 offset의 정보가 들어있다. value에는 splitting되어서 넘어온 데이터의 전체가 text로 들어있다.**
즉, 전체 txt에서 현재 value의 offset을 확인하려면 key를 봐야하고 여기서 단어별로 쪼개면 offset과 value를 쪼갠 length를 이용해서 위치를 찾아야 할 것 같다!

넘어온 value Text를 stringtokenizer로 만들어서 itr에 넣어줌

```
StringTokenizer itr = new StringTokenizer(value.toString(), " ",true); // 공백을 포함하고 공백을 기준으로 쪼갠다 -> A B C 라면 [A ],[B ],[C ]이렇게 쪼갬
```

**StringTokenizer(String str, String delim, boolean returnDelims);**
str을 delim으로 쪼개는데 그 delim까지 포함시킬지 returnDelims로 정한다.

넘어온 key에서 offset을 뽑아냄

```
Long position = ((LongWritable)key).get();
```

offset인 key는 LongWritable타입이다.

이제 현재 첫문자의 offset과 쪼개진 String이 준비중이다! 단어별로 현재 위치를 찾아보자!

```
while ( itr.hasMoreTokens() ) {
//               Text word = new Text();
//               Text resVal = new Text();
               String token = itr.nextToken();
               word.set(token.trim());
               if(!token.equals(" ")){
                   resVal.set(filename+":"+position);
                   context.write(word,resVal);
               }

               position += token.length();

           }
```

Token이 있다면 while을 계속돈다.
key로 넘겨줄 단어를 저장할 Text를 word로 지정
value로 넘겨줄 파일정보:위치 Text를 resVal로 지정
itr.nextToken()으로 토큰을 받아온걸 token에 넣어줌(공백포함 문자)
key로 넘겨줄 단어를 token에서 공백을 제거하고 word에 set한다.
만약 token이 공백이 아니라 문자가 맞다면 context.write(word,resVal)한다
여기서 resVal에 들어갈 파일정보와 위치는?

먼저 파일정보는 mapper가 끝날때 까지 동일하기 때문에 전역변수로 설정

```
private String filename;

       protected void setup(Context context) throws IOException, InterruptedException {
           filename = ((FileSplit)context.getInputSplit()).getPath().getName();
       }
```

setup은 앞에서도 말했지만 **mapper와 reducer가 돌기전에 가장 먼저한다.**

```
((FileSplit)context.getInputSplit()).getPath().getName();
```

**파일의 정보를 가져온다. 외워!**

파일의 정보는 가져왔다 그렇다면 위치는?

```
Long position = ((LongWritable)key).get();
position += token.length();
```

뽑아낸 offset인 position에서 단어하나가 들어갈때마다 단어의 길이만큼 더해준다. 즉, A BB CCC 의 경우 [A ]로 길이는 2 그다음이 3이니까 [BB ]문자가 시작되는 지점인 3으로 position이 설정된다.
이렇게 mapper를 설정하면 출력 파일로 현재 key(단어)가 value(현재파일:위치)를 가지고 출력된다.
**shuffling**을 거치면 key(단어)당 존재하는 모든 파일과 위치를 가진 value-list를 가진 key,value-list 가 출력된다.

reducer는 입력값을 그대로 파일로 만들기만하면 된다.

```
public static class ConcatenatorReducer
           extends Reducer<Text,Text,Text,Text> {

       // variables
       private Text result = new Text();

       // key : a disticnt word
       // values :  Iterable type (data list)
       public void reduce(Text key, Iterable<Text> values, Context context)
               throws IOException, InterruptedException {

           String sum = "";
           for ( Text val : values ) {
               sum += val.toString()+" ,";
           }
           sum = sum.substring(0,sum.length()-1);
           result.set(sum);
           context.write(key,result);
       }
   }
```

그대로 넘겨주면 되지만 list로 넘어오는 Text를 다시 string으로 합치고 다시 text로 set해서 보내야한다....

```
Reducer<Text,Text,Text,Text>
```

input : Text(단어),Text(현재파일:위치)
output : Text(단어),Text(현재파일:위치)

넘어오는 list를 받아서 String으로 만들자(이때 reduce는 한줄마다 실행(A [현재파일:현재위치, ..........])즉 value를 전체다 받아서 돌려주면된다.)

```
public void reduce(Text key, Iterable<Text> values, Context context)
               throws IOException, InterruptedException
```

```
String sum = "";
for ( Text val : values ) {
               sum += val.toString()+" ,";
           }
```

모든 value의 값을 받아서 toString()하고 ,를 추가해준다. 이렇게 하면 제일 마지막 단어도 ,가 추가되는데 length-1을 해줘서 없애주자

```
sum = sum.substring(0,sum.length()-1);	// 전체 length에서 1을 빼준 String을 넣어줘야 마지막에는 ,이 빠진다.
           result.set(sum);
           context.write(key,result);
```

**java**
.split("\t"); 탭기준 쪼개기 => 배열생성
.indexOf(""); 특정 문자인덱스찾기
.substring(,); 앞 인덱스부터 뒤 숫자만큼 문자열자르기

```
public static void main(String[] args) throws Exception {
       Configuration conf = new Configuration();
       String[] otherArgs = new GenericOptionsParser(conf,args).getRemainingArgs();
       if ( otherArgs.length != 2 ) {
           System.err.println("Usage: <in> <out>");
           System.exit(2);
       }
       FileSystem hdfs = FileSystem.get(conf); //outdirectory auto delete
       Path output = new Path(otherArgs[1]);
       if (hdfs.exists(output)){
           hdfs.delete(output, true);
       }
       Job job = new Job(conf,"InvertedIndex");
       job.setJarByClass(InvertedIndex.class);

       // let hadoop know my map and reduce classes
       job.setMapperClass(TokenizerMapper.class);
       // job.setCombinerClass(IntSumReducer.class);
       job.setReducerClass(ConcatenatorReducer.class);

       job.setOutputKeyClass(Text.class);
       job.setOutputValueClass(Text.class);

       // set number of reduces
       job.setNumReduceTasks(2);

       // set input and output directories
       FileInputFormat.addInputPath(job,new Path(otherArgs[0]));
       FileOutputFormat.setOutputPath(job,new Path(otherArgs[1]));
       System.exit(job.waitForCompletion(true) ? 0 : 1 );
   }
```

자동으로 outputdirectory를 삭제하기 위해서 사용하려면 import FileSystem 해야함

```
FileSystem hdfs = FileSystem.get(conf); //outdirectory auto delete
       Path output = new Path(otherArgs[1]);
       if (hdfs.exists(output)){
           hdfs.delete(output, true);
       }
```

mapper와 reducer 클래스

```
job.setMapperClass(TokenizerMapper.class);
job.setReducerClass(ConcatenatorReducer.class);
```

reduce의 output결과.

```
job.setOutputKeyClass(Text.class);
job.setOutputValueClass(Text.class);
```

reducer갯수

```
job.setNumReduceTasks(2);
```

#### Matrix Addition연산

matrix 덧셈
input txt 구조
A 0 0 3 -> 0,0에 3의 값
즉, map될때마다 tab을 기준으로 value를 자르고 배열로 만들어서 행,열의 정보와 value의 정보를 가져온다.

전체코드

```
package ssafy;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class MatrixAdd {
 public static class MAddMapper  extends  Mapper<Object, Text, Text , IntWritable>{

    public void map(Object key, Text value, Context context)
 throws IOException, InterruptedException {
      Text outKey = new Text();
      IntWritable outVal = new IntWritable();
      String[] arr = value.toString().split("\t");
      outKey.set(arr[1]+"\t"+arr[2]);
      outVal.set(Integer.parseInt(arr[3]));

      context.write(outKey,outVal);


    }
 }
 public static class  MAddReducer  extends Reducer<Text, IntWritable, Text, IntWritable> {
   public void reduce(Text  key, Iterable<IntWritable> values, Context  context)
 throws IOException, InterruptedException {
     int sum = 0;
     for(IntWritable val : values){
       sum += val.get();
     }
     context.write(key,new IntWritable(sum));




   }
 }

 public static void main(String[] args) throws Exception {
   Configuration conf = new Configuration();
   String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
   if (otherArgs.length != 2) {  // �hadoop jar jarname.jar ������ � ��� ��� ��
     System.err.println("Usage: <in> <out>");
     System.exit(2);
   }
   FileSystem hdfs = FileSystem.get(conf);
   Path output = new Path(otherArgs[1]);
   if (hdfs.exists(output))
           hdfs.delete(output, true);

   Job job = new Job(conf, "matrix addition");
   job.setJarByClass(MatrixAdd.class);   // class � ��
   job.setMapperClass(MAddMapper.class);                // Map class ��
   job.setReducerClass(MAddReducer.class);                // Reduce class ��
   job.setOutputKeyClass(Text.class);    // output key type ��
   job.setOutputValueClass(IntWritable.class);   // output value type ��
   job.setNumReduceTasks(2);     // ��� ���� reduce�� ��

   FileInputFormat.addInputPath(job, new Path(otherArgs[0]));  // ���� ���� ��
   FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));  // ���� ���� ��
   System.exit(job.waitForCompletion(true) ? 0 : 1); // ��
 }
}
```

먼저 mapper로 들어오는 txt들을 map에 따라서 정제해보자

```
public static class MAddMapper  extends  Mapper<Object, Text, Text , IntWritable>{

    public void map(Object key, Text value, Context context)
 throws IOException, InterruptedException {
      Text outKey = new Text();
      IntWritable outVal = new IntWritable();
      String[] arr = value.toString().split("\t");
      outKey.set(arr[1]+"\t"+arr[2]);
      outVal.set(Integer.parseInt(arr[3]));

      context.write(outKey,outVal);


    }
 }
```

input으로 들어오는 정보를 tab을 기준으로 자르고 String배열에 넣자

```
String[] arr = value.toString().split("\t");
```

필요한 행렬 정보는 key(Text)로 value(IntWritable)로 보낸다.

```
outKey.set(arr[1]+"\t"+arr[2]);
outVal.set(Integer.parseInt(arr[3]));
context.write(outKey,outVal);
```

이제 reducer에서 리스트로 받은 value를 모두 합치게 되면 동일한 행열의 정보에서 가진 값들을 합친 matrix add가 완성된다.

```
public static class  MAddReducer  extends Reducer<Text, IntWritable, Text, IntWritable> {
   public void reduce(Text  key, Iterable<IntWritable> values, Context  context)
 throws IOException, InterruptedException {
     int sum = 0;
     for(IntWritable val : values){
       sum += val.get();
     }
     context.write(key,new IntWritable(sum));
   }
 }
```

받은 리스트를 돌면서 int에 저장하고 저장한 int값을 출력할때 생성한 IntWritable객체 에 set 한다.

```
int sum = 0;
for(IntWritable val : values){
	sum += val.get(); // IntWritable을 int로
}
context.write(key,new IntWritable(sum)); // IntWritable객체로 생성해서 바로 넣어줘도 된다.
```

main은 출력값과 class에 맞게 수정한다

```
public static void main(String[] args) throws Exception {
   Configuration conf = new Configuration();
   String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
   if (otherArgs.length != 2) {  // �hadoop jar jarname.jar ������ � ��� ��� ��
     System.err.println("Usage: <in> <out>");
     System.exit(2);
   }
   FileSystem hdfs = FileSystem.get(conf);
   Path output = new Path(otherArgs[1]);
   if (hdfs.exists(output))
           hdfs.delete(output, true);

   Job job = new Job(conf, "matrix addition");
   job.setJarByClass(MatrixAdd.class);   // class � ��
   job.setMapperClass(MAddMapper.class);                // Map class ��
   job.setReducerClass(MAddReducer.class);                // Reduce class ��
   job.setOutputKeyClass(Text.class);    // output key type ��
   job.setOutputValueClass(IntWritable.class);   // output value type ��
   job.setNumReduceTasks(2);     // ��� ���� reduce�� ��

   FileInputFormat.addInputPath(job, new Path(otherArgs[0]));  // ���� ���� ��
   FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));  // ���� ���� ��
   System.exit(job.waitForCompletion(true) ? 0 : 1); // ��
 }
```

중간에 끼워넣는 hadoop_command

```
hdfs dfs -put 보낼데이터경로 받을경로
```

input데이터를 넣을 때 사용함

---

Day3

**reduce함수가 호출되면 value로 들어온걸 다 메모리에 넣어놓는다 꼭 기억해야함!!! 1-Phase와 2-Phase의 차이때문**

Matrix Multiplication
행렬의 곱
![img](http://db.kockoc.com/lec/Hard/ga/eq5.gif)
A행렬 x B행렬 일 경우
A nk \* B km = C nm 이다 즉, A행렬의 행의 번호와 B행렬의 열의 번호가 C에 영향을 미친다.
그리고 곱을 하려면 A의 열의 개수와 B의 행의 개수가 같아야 한다. => k

#### 1-Phase 알고리즘

A행렬의 i행 p열
key(i,1),(i,2)....(i,m)
value(p,A ip)

B행렬의 p행 j열
key(1,j),(2,j),...(n,j)
value(p,B pj)

A 1 1 3 -> map함수실행
A 1 2 -5 -> map함수실행
A 2 1 6 -> map함수실행
A 2 2 12 -> map함수실행
B 1 1 2 -> map함수실행
B 1 2 11 -> map함수실행
B 2 1 1 -> map함수실행
B 2 2 -7 -> map함수실행

A mapping 이후
key(1,1) value(1,3)
key(1,2) value(1,3)

key(1,1) value(2,-5)
key(1,2) value(2,-5)

key(2,1) value(1,6)
key(2,2) value(1,6)

key(2,1) value(2,12)
key(2,2) value(2,12)

B mapping 이후
key(1,1) value(1,2)
key(2,1) value(1,2)

key(1,2) value(1,11)
key(2,2) value(1,11)

key(1,1) value(2,1)
key(2,1) value(2,1)

key(1,2) value(2,-7)
key(2,2) value(2,-7)

shuffling 이후
key(1,1) value<(1,3),(2,-5),(1,2),(2,1)> -> reduce함수실행, value-list를 메모리에 넣어놓음
key(1,2) value<(1,3),(2,-5),(1,11),(2,-7)> -> reduce함수실행
key(2,1) value<(1,6),(2,12),(1,2),(2,1)> -> reduce함수실행
key(2,2) value<(1,6),(2,12),(1,11),(2,-7)> -> reduce함수실행

reducer 이후
key(1,1) value(3*2 + -5*1)
key(1,2) value(3*11 + -5*-7)
key(2,1) value(6*2 + 12*1)
key(2,2) value(6*11 + 12*-7)

#### 1-Phase start int 2-Phase 알고리즘

A 1 1 3 -> map함수실행
A 1 2 -5 -> map함수실행
A 2 1 6 -> map함수실행
A 2 2 12 -> map함수실행
B 1 1 2 -> map함수실행
B 1 2 11 -> map함수실행
B 2 1 1 -> map함수실행
B 2 2 -7 -> map함수실행

A mapping 이후
key(1,1,1) value(3)
key(1,2,1) value(3)

key(1,1,2) value(-5)
key(1,2,2) value(-5)

key(2,1,1) value(6)
key(2,2,1) value(6)

key(2,1,2) value(12)
key(2,2,2) value(12)

B mapping 이후
key(1,1,1) value(2)
key(2,1,1) value(2)

key(1,2,1) value(11)
key(2,2,1) value(11)

key(1,1,2) value(1)
key(2,1,2) value(1)

key(1,2,2) value(-7)
key(2,2,2) value(-7)

suffling이후
key(1,1,1) value(<3,2>)
key(1,2,1) value(<3,11>)

key(1,1,2) value(<-5,1>)
key(1,2,2) value(<-5,-7>)

key(2,1,1) value(<6,2>)
key(2,2,1) value(<6,11>)

key(2,1,2) value(<12,1>)
key(2,2,2) value(<12,-7>)

reduce이후
key(1,1) value(3*2)
key(1,2) value(3*11)

key(1,1) value(-5*1)
key(1,2) value(-5*-7)

key(2,1) value(6*2)
key(2,2) value(6*11)

key(2,1) value(12*1)
key(2,2) value(12*-7)

##### 2-Phase start in 2-Phase알고리즘

map input
key(1,1) value(3*2)
key(1,2) value(3*11)

key(1,1) value(-5*1)
key(1,2) value(-5*-7)

key(2,1) value(6*2)
key(2,2) value(6*11)

key(2,1) value(12*1)
key(2,2) value(12*-7)

map output 동일하다
key(1,1) value(6)
key(1,2) value(33)

key(1,1) value(-5)
key(1,2) value(35)

key(2,1) value(12)
key(2,2) value(66)

key(2,1) value(12)
key(2,2) value(-84)

shuffling 이후

key(1,1) value(6,-5)
key(1,2) value(33,35)
key(2,1) value(12,12)
key(2,2) value(66,-84)

reduce 이후

key(1,1) value(6+-5)
key(1,2) value(33+35)
key(2,1) value(12+12)
key(2,2) value(66+-84)

**중요한건 reduce함수가 호출되면서 있던 value-list의 개수차이이다. 2-Phase 알고리즘을 이용하면 무조건 value-list에는 곱해지는 행렬의 개수 2개만큼만(1-Phase)그리고 A의 열의 개수=B의 행의 개수 만큼(대신 덧셈임) 들어가기 때문에 모든 행렬을 메모리에 넣는 1-Phase알고리즘과 메모리차이가 매우크다.**

#### 1-Phase 연습

전체코드

```
package ssafy;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class MatrixMulti {
   // Map
   public static class MMMapper extends Mapper<Object, Text, Text, Text>{
               private String Matrix1name;
               private String Matrix2name;

       private int n;  // number of rows in matrix A
       private int l;  // number of columns in matrix A
       private int m;  // number of columns matrix B
       protected void setup(Context context) throws IOException, InterruptedException {
           Configuration config = context.getConfiguration();
           Matrix1name = config.get("Matrix1name","A");
           Matrix2name = config.get("Matrix2name","B");
           n = config.getInt("n",5);
           l = config.getInt("l",5);
           m = config.getInt("m",5);

       }
       public void map(Object key, Text value, Context context
               ) throws IOException, InterruptedException {
           // StringTokenizer token = new StringTokenizer(value.toString());
           String[] arr = value.toString().split("\t");
                   if (arr[0].equals(Matrix1name)) {
                       for (int i = 0; i < m; i++) {
                           Text resKey = new Text(""+arr[1]+","+i);
                           Text resVal = new Text(""+arr[2]+" "+arr[3]);
                           context.write(resKey,resVal);
                       }
                   }else if(arr[0].equals(Matrix2name)){
                       for (int i = 0; i < n; i++) {
                           Text resKey = new Text(""+i+","+arr[2]);
                           Text resVal = new Text(""+arr[1]+" "+arr[3]);
                           context.write(resKey,resVal);
                       }
                   }
       }
   }
   // Reduce
   public static class MMReducer extends Reducer<Text, Text, Text, Text> {

       public void reduce(Text key, Iterable<Text> values, Context context)
           throws IOException, InterruptedException {
               for(Text val : values){
                   context.write(key,val);
               }
       }
   }
   // Main
   public static void main(String[] args) throws Exception {
       Configuration conf = new Configuration();
       String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
       if (otherArgs.length != 7) {
           System.err.println("Usage: <Matrix 1 name> <Matrix 2 name> <Number of rows in Matrix 1><Number of columns in Matrix 1 (i.e., Number of rows in Matrix 2)> <Number of columns in Matrix 2> <in> <out>");
           System.exit(2);
       }

               FileSystem hdfs = FileSystem.get(conf);
               Path output = new Path(otherArgs[6]);
               if (hdfs.exists(output))
                       hdfs.delete(output, true);

       Job job = new Job(conf, "matrix multiplication prepare");
       Configuration config = job.getConfiguration();
               config.set("Matrix1name", otherArgs[0]);
               config.set("Matrix2name", otherArgs[1]);
       config.setInt("n",Integer.parseInt(otherArgs[2]));
       config.setInt("l",Integer.parseInt(otherArgs[3]));
       config.setInt("m",Integer.parseInt(otherArgs[4]));


       job.setJarByClass(MatrixMulti.class);
       job.setMapperClass(MMMapper.class);
       job.setReducerClass(MMReducer.class);
       job.setMapOutputKeyClass(Text.class);
       job.setMapOutputValueClass(Text.class);
       job.setOutputKeyClass(Text.class);
       job.setOutputValueClass(Text.class);
       job.setNumReduceTasks(2);

       FileInputFormat.addInputPath(job, new Path(otherArgs[5]));
       FileOutputFormat.setOutputPath(job, new Path(otherArgs[6]));
       System.exit(job.waitForCompletion(true) ? 0 : 1);
   }
}
```

setup 분석

```
protected void setup(Context context) throws IOException, InterruptedException {
           Configuration config = context.getConfiguration();
           Matrix1name = config.get("Matrix1name","A");
           Matrix2name = config.get("Matrix2name","B");
           n = config.getInt("n",5);
           l = config.getInt("l",5);
           m = config.getInt("m",5);

       }
```

main에서

```
Configuration config = job.getConfiguration();
               config.set("Matrix1name", otherArgs[0]);
               config.set("Matrix2name", otherArgs[1]);
       config.setInt("n",Integer.parseInt(otherArgs[2]));
       config.setInt("l",Integer.parseInt(otherArgs[3]));
       config.setInt("m",Integer.parseInt(otherArgs[4]));
```

이렇게 args로 행렬의 각각의 이름과 A행렬의 행,열, B행렬의 열 을 받는다.
만약 입력되지않으면 default로 A,B,5,5,5를 받기로 한다.

map 분석

```
public void map(Object key, Text value, Context context
               ) throws IOException, InterruptedException {
           // StringTokenizer token = new StringTokenizer(value.toString());
           String[] arr = value.toString().split("\t");
                   if (arr[0].equals(Matrix1name)) {
                       for (int i = 0; i < m; i++) {
                           Text resKey = new Text(""+arr[1]+","+i);
                           Text resVal = new Text(""+arr[2]+" "+arr[3]);
                           context.write(resKey,resVal);
                       }
                   }else if(arr[0].equals(Matrix2name)){
                       for (int i = 0; i < n; i++) {
                           Text resKey = new Text(""+i+","+arr[2]);
                           Text resVal = new Text(""+arr[1]+" "+arr[3]);
                           context.write(resKey,resVal);
                       }
                   }
       }
```

A 1 1 3 이런식으로 값이 들어오는데 tab을 기준으로 split해서 string배열에 저장
배열을 돌면서 A행렬일때와 B행렬일때 만들어주는 key와 value가 다르다.
그리고 **new Text()에서 ""를 앞에 붙여줘야 Text로 생성된다.**
각각 한줄씩 map함수가 돌면서 1줄에 2개씩나오는(2\*2라서) key,value쌍을 map함수의 출력으로 보낸다.

reduce 분석
원래 1-Phase 알고리즘은 suffling을 통해서 나온 value-list를 분석해서 첫번째 value-list 값이 같으면 곱하고 전체 value-list에서 나온 최종값을 더해야 하는데 여기서는 그냥 value-list를 하나씩 쪼개서 출력만 하기로 했다.

```
public static class MMReducer extends Reducer<Text, Text, Text, Text> {

       public void reduce(Text key, Iterable<Text> values, Context context)
           throws IOException, InterruptedException {
               for(Text val : values){
                   context.write(key,val);
               }
       }
   }
```

---

Day4

All Pair Partition Algorithm
