하둡을 사용해서 병렬처리?? -> 빅데이터 분석 문제들에 대해서 맵리듀스 알고리즘을 자바언어로 구현하고 실행한다.

Scale-out 아주 많은 값싼 서버들을 이용함
Scale-up 적은 수의 값비싼 서버들을 이용함

데이터 중심 어플리케이션 분야에서는 아주 많은 값싼 서버들을 많이 이용하는 것을 선호함
고가의 서버들은 가격에 관점에서는 선형으로 성능이 증가하지 않음 -> 두 배의 성능의 프로세서 한 개를 가진 컴퓨터의 가격이 일반적인 프로세서 한 개를 가진 컴퓨터 가격의 두 배보다 훨씬 더 비쌈

맵리듀스 프레임워크는 많은 컴퓨터를 묶어서 처리한다. 각각의 머신에서 동시에 mapping 한다(각각의 머신 안에서는 한줄씩 map함수가 실행되어서 직렬처럼 보이지만)-> 병렬처리

데이터 중심 프로세싱

- 한대의 컴퓨터의 능력으로 처리가 어려움
- 근본적으로 수 십대, 수백대 혹은 수 천대의 컴퓨터를 묶어서 처리해야함
- 맵리듀스 프레임 워크가 하는 것이 바로 이것
  맵리듀스는 빅데이터를 이용한 효율적인 계산이 가능한 첫 번째 프로그래밍 모델
  구글의 맵리듀스 또는 오픈소스인 하둡은 맵리듀스 프레임워크의 우수한 구현의 형태임

MapReduce Programming Model -> 함수형 프로그래밍 언어의 형태
유저는 아래 3가지 함수를 구현해서 제공해야함
Main 함수
Map 함수 : (key1, val1) -> [(key2, val2)]
Reduce 함수 : (key2, [val2]) -> [(key3, val3)]

맵리듀스 프레임워크에서는 각각의 레크드 또는 튜플은 키-밸류 쌍으로 표현된다.

맵리듀스 페이즈는 3단계로 수행되는데,

아래에서 자세히

1. 맵 페이즈는 제일 먼저 수행되며 데이터의 여러 파티션에 병렬 분산으로 호출되어 수행된다.
   각 머신마다 수행된 mapper는 맵 함수가 입력 데이터릐 한 줄 마다 맵 함수를 호출한다.
2. 셔플링 페이즈는 모든 머신에서 맵 페이즈가 다 끝나면 시작된다.
   맵 페이즈에서 각각의 머신으로 보내진 key, value 쌍을 key 를 이용해서 sorting 한 후 에 각각의 key마다 같은 key를 가진 key, value 쌍을 모아서 밸류리스트를 만든 다음에 key, value-list 형태로 key에 따라서 여러 머신에 분산해서 보낸다.
3. 리듀스페이즈 에서는 셔플링 페이즈가 다 끝나면 각 머신마다 리듀스 페이즈가 시작되는데 각각의 머신에서는 셔플링 페이즈에서 해당 머신으로 보내진 각각의 쌍 마다 리듀스 함수가 호출되며 하나의 리듀스 함수가 끝나면 다음 쌍에 리듀스 함수가 호출된다.
   출력이 있다면 key, value 쌍의 형태로 출력된다.

각각의 document에서 mapper로 key value를 만든 다음에 (중복된 key) 특정 조건에 따라서 각각을 머신으로 보내고 (반드시 동일한 키는 동일한 머신으로 ) sorting 한다. 이후에 셔플링 페이즈에서 key값에 따라서 value 를 리스트로 만든다 (각각의 머신마다 발생) 리듀스 페이즈에서 동일한 key 의 value list를 값으로 만든다.

하둡 분산 파일 시스템(Hadoop Distributed File System - HDFS)

- 빅 데이터 파일을 여러 대의 컴퓨터에 나누어서 저장함
- 각 파일은 여러 개의 순차적인 블록으로 저장함
- 하나의 파일의 각각의 블록은 폴트 톨러런스(fault tolerance)를 위해서 여러 개로 복사되어 여러 머신의 여기저기 저장됨
  폴트 톨러런스는 시스템을 구성하는 부품의 일부에서 결함 또는 고장이 발생하여도 정상적 혹은 부분적으로 기능을 수행할 수 있는 것을 말함

#### Combine 함수

Map 함수의 결과 크기를 줄여준다. -> 셔플링 비용을 줄여준다.
각각의 머신에서 map의 결과를 combine 해서 key 에 따른 value값을 미리 합쳐주고 셔플링으로 보낸다.

---

#### Map함수

org.apache.hadoop.mapreduce라는 패키지에 있는 Mapper 클래스를 상속받아서 맵 메소드를 수정한다.

```
public static class TokenizerMapper
			extends Mapper<Object,Text,Text,IntWritable> {

		// variable declairations
		private final static IntWritable one = new IntWritable(1);
		private Text word = new Text();

		// map function (Context -> fixed parameter)
		public void map(Object key, Text value, Context context)
				throws IOException, InterruptedException {

			// value.toString() : get a line
			StringTokenizer itr = new StringTokenizer(value.toString());
			while ( itr.hasMoreTokens() ) {
				word.set(itr.nextToken());

				// emit a key-value pair
				context.write(word,one);
			}
		}
	}

```

여기서 Object key에는 입력 텍스트 파일에서 맨 앞 문자를 기준으로 맵 함수가 호출된 해당 라인의 첫 번째 문자까지의 오프셋(매 라인마다 map이 호출된다고 했는데 그런 호출라인의 정보)
Text value에는 텍스트 해당 라인 전체가 들어있다.

#### Reduce함수

org.apache.hadoop.mapreduce라는 패키지에 있는 Reducer 클래스를 상속받아서 reduce 메소드를 수정한다.

```
public static class IntSumReducer
			extends Reducer<Text,IntWritable,Text,IntWritable> {

		// variables
		private IntWritable result = new IntWritable();

		// key : a disticnt word
		// values :  Iterable type (data list)
		public void reduce(Text key, Iterable<IntWritable> values, Context context)
				throws IOException, InterruptedException {

			int sum = 0;
			for ( IntWritable val : values ) {
				sum += val.get();
			}
			result.set(sum);
			context.write(key,result);
		}
	}

```

셔플링 페이즈의 출력을 입력으로 받는데 key, value-list 형태 -> Text key , Iterable<IntWritable> values
value-list는 맵 함수의 출력에서 key를 갖는 key,value 쌍들의 value들의 리스트

---

맵 리듀스의 3페이즈(map, shuffle, reduce)

1. 각각의 머신에서 documents를 mapping의 map을 사용해서 한줄씩 map함수를 실행하면 각각의 머신에서 key와 value를 가진 key,value쌍이 나온다. 각각의 머신에서 따로 처리 하기때문에 병렬처리 이다.
   그리고 처리된결과를 mapreduce framework가 머신에 보내준다.(같은 키는 같은 머신으로 보내주자) 여기까지가 map phase
2. 셔플링페이즈에서는 map에서 나온 key,value 를 sorting하고 각 key마다 그 key 를 가진 value들을 모아서 value-list를 만든다. 즉, (key, value-list) 모양을 만든다.
3. 이렇게 만들어진 key,value-list 각각마다 reduce함수가 하나씩 호출된다. 그 결과로 key, value 쌍이 만들어진다.

만약 combine이 있다면 map의 output을 바로 shuffling phase로 보내는 것이 아니라 중복 key에 따른 value를 합친 결과 key, value를 보낸다.

정리 : map의 결과 key,value 쌍의 리스트가 만들어짐, shuffling의 결과 key,value-list 쌍의 리스트가 만들어짐, reduce 는 key,value-list 쌍의 리스트를 입력으로 받고 key,value로 만들어준다.

---

### 하둡의 설치와 사용법 (window 기준)

VMware 최신 버전 설치 -> Ubuntu 최신 버전 설치 -> Hadoop 최신 버전 설치

VMware 설치

```
https://www.vmware.com/products/workstation-player.html
```

VMware에 Ubuntu설치

```
http://www.ubuntu.com/download/ubuntu/download
```

iso파일이 설치가 되면 vmware에서 create New Machine

Full name은 자유롭게 ex) ubuntu
User name : hadoop

ubuntu에서 terminal 접속

```
$ wget http://kdd.snu.ac.kr/~kddlab/Project.tar.gz
$ tar zxf Project.tar.gz
$ sudo chown -R hadoop:hadoop Project
$ cd Project
$ sudo mv hadoop-3.2.2 /usr/local/hadoop
$ sudo apt update
$ sudo apt install ssh openjdk-8-jdk ant -y
$ ./set_hadoop_env.sh
$ source ~/.bashrc

```

hadoop 계정에서 ssh key generation(리눅스에서 하둡을 돌리려면 pwd가 필요한데 안치고도 돌릴수 있도록 ssh키 생성)

```
$ ssh-keygen -t rsa -P ""
```

저장할 파일을 물어보면 default로 enter
생성된 id_rsa.pub을 authorized_keys에 넣는다.
직금은 하나지만 각 서버에 있는 모든 파일을 하나의 authorized_keys에 모은다.

```
$ cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys
```

제대로 생성되었는지 확인

```
$ ssh localhost
```

질문이 뜨면 yes를 입력하고 그 다음 비밀번호를 물어보지 않고 prompt가 뜨면 성공

만약 제대로 등록하였는데 계속 비밀번호를 입력하라고 한다면

```
$ restorecon -R -v /root/.ssh
```

Path를 지정하기 위해

```
$ cd /home/hadoop
$ source .bashrc
```

Name node format(Disk format과 같은 개념)

```
$ hadoop namenode -format
```

Dfs daemon start

```
$ start-dfs.sh
```

MapReduce daemon start(standalone 모드(컴퓨터한대)에서는 불필요)

```
$ start-mapred.sh
```

확인

```
$ jps
```

SecNameNode, ondaryNameNode, DateNode, TaskTracker(Standalone에서 불필요), JobTracker(Standalone에서 불필요)

이렇게 생성하면 리눅스 디렉토리와 hadoop 디렉토리가 있다.
src/ Driver.java(맵리듀스 코드 컴파일을 위한 파일)

hadoop(HDFS)에는 디렉토리를 만들어 줄 것 이다.

만든 코드를 실행하려면 맵리듀스 코드 컴파일을 위해서 만들어 놓은 Driver.java 파일을 수정해야 한다.

```
pgd.add("사용명(보통 자바파일명)", 자바파일명.class, "comment");
```

**Driver.java 파일이 바뀌면 반드시 ant를 다시 수행해야함**

```
$ cd ~/Project
$ ant
- 유닉스의 make 같은것으로 java개발환경에서의 표준 빌드 도구
- 자바 파일을 컴파일하는 javac도 있지만, ant 는 여러 dependency를 고려하여 소스피일을 컴파일
- src 디렉토리에 있는 것을 다 모아서 컴파일 한 후에 jar를 만든다.
- build.xml에 정의한 대로 수행된다.
```

그리고 위에서 설정한 자바 파일과

```
hadoop jar buildxml에 정의한이름.jar 사용명(보통 자바파일명) 하둡의input디렉토리 하둡의output디렉토리
```

일치 시켜줘야함

데이터 테스터를 HDFS에 넣어야 한다.

```
$ cd /home/hadoop/Project
$ hdfs dfs -mkdir wordcount_test (만약 디렉토리가 없다고 나오면 오류md 참고)
- 하둡의 HDFS에 word_test 디렉토리를 생성
$ hdfs dfs -put data/wordcount-data.txt wordcount_test
- Linux의 data 디렉토리에 있는 wordcount-data.txt 파일을 하둡의 HDFS의 wordcount_test디렉토리로 보냄
```

**반드시 맵리듀스 프로그램이 결과를 저장할 디렉토리를 삭제한 후 프로그램을 실행해야함**

```
$ hdfs dfs -rm -r word_test_out (하둡의 output디렉토리)
```

Wordcount MapReduce 알고리즘 코드 실행

```
$ cd /home/hadoop/Project
$ hadoop jar ssafy.jar wordcount wordcount_test wordcount_test_out
- Driver.java에 표시한대로 wordcount를 써서 Wordcount 맵 리듀스 코드를 수행
- Wordcount_test 디렉토리에 들어있는 모든 파일을 맵 함수의 입력으로 사용함
```

하둡의 실행방법

```
$ hadoop jar jarfile programname inputDirectory outputDirectory
```

결과확인
reducer개수에 따라서 출력파일이 달라진다. 2개인경우

```
$ hdfs dfs -cat wordcount_test_out/part-r-00000 | more
- 0번 reducer가 출력한 파일의 내용을 보여줌
- more 을 해야 안짤리고 계속 볼수있다.

$ hdfs dfs -cat wordcount_test_out/part-r-00001 | more
- reducer의 개수 -1개 까지 넣어서 보면된다.
```

---

#### 하둡의 맵리듀스의 map함수, reduce함수, combine함수 등에서 입출력의 파라미터로 쓰는 클래스와 자바 타입

- Text : String
- IntWritable : Int
- LongWritable : long
- FloatWritable : float
- DoubleWritable : double

만약 새로운 타입의 클래스를 정의해서 입출력에 사용하고 싶다면 필요한 여러 함수도 함께 정의를 해주어야한다.

---

#### Wordcount.java 를 살펴보자

```
package ssafy; //반드시 build.xml에서 사용한걸 사용

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class Wordcount {
	/*
	Object, Text : input key-value pair type (always same (to get a line of input file))
	Text, IntWritable : output key-value pair type
	*/
	public static class TokenizerMapper
			extends Mapper<Object,Text,Text,IntWritable> {	//Mapper를 상속받는다. map함수의 입출력 : 편의상 Object로 넣었고 key, Text value 형태로 입력 된다.  출력은 Text key, IntWritable value 이다.

		// variable declairations
		private final static IntWritable one = new IntWritable(1); // 변수이름이 one 인 intwritable을 생성하고 1의 값을 고정으로 넣는다.
		private Text word = new Text();

		// map function (Context -> fixed parameter)
		public void map(Object key, Text value, Context context)
				throws IOException, InterruptedException {

			// value.toString() : get a line
			StringTokenizer itr = new StringTokenizer(value.toString()); // value는 String이 아니라서 toString해줘야함. String 을 Tokenizer화 해준다.
			while ( itr.hasMoreTokens() ) {// 더이상 토큰이 없을때 까지
				word.set(itr.nextToken());// Text타입으로 만든 word에 값을 set 해준다.

				// emit a key-value pair
				context.write(word,one);// 각각의 Text와 1을 넣어준다. (key, value)
			}
		}
	}

	/*
	Text, IntWritable : input key type and the value type of input value list
	Text, IntWritable : output key-value pair type
	*/
	public static class IntSumReducer
			extends Reducer<Text,IntWritable,Text,IntWritable> {// reducer를 상속해서 만든다. input은 Text, IntWritable ouput도 동일

		// variables
		private IntWritable result = new IntWritable();

		// key : a disticnt word
		// values :  Iterable type (data list)
		public void reduce(Text key, Iterable<IntWritable> values, Context context) // intwritable타입의 리스트
				throws IOException, InterruptedException {

			int sum = 0;
			for ( IntWritable val : values ) {
				sum += val.get();
			}
			result.set(sum);
			context.write(key,result);
		}
	}


	/* Main function */
	public static void main(String[] args) throws Exception {
		Configuration conf = new Configuration();
		String[] otherArgs = new GenericOptionsParser(conf,args).getRemainingArgs();
		if ( otherArgs.length != 2 ) {
			System.err.println("Usage: <in> <out>");
			System.exit(2);
		}
		Job job = new Job(conf,"word count"); //job 작성, 따옴표안은 설명을 쓰면됨
		job.setJarByClass(Wordcount.class); //job을 수행할 class 선언, 파일명.class, 대소문자주의

		// let hadoop know my map and reduce classes
		job.setMapperClass(TokenizerMapper.class); //Map class 선언, 위에서 작성한 class명
		//job.setCombinerClass(IntSumReducer.class); //Combiner class 선언, reducer함수의 클래스를 넣는다.
		job.setReducerClass(IntSumReducer.class); //Reduce class 선언

		job.setOutputKeyClass(Text.class); //Output key type 선언, reduce의 key type
		job.setOutputValueClass(IntWritable.class); //Output key value 선언, reduce의 value type

		//job.setMapOutputKeyClass(Text.class); //Map은 Output key type가 다르다면 선언
		//job.setMapOutputKeyClass(IntWritable.class); //Map은 Output value type가 다르다면 선언

		// set number of reduces
		job.setNumReduceTasks(2); //동시에 수행되는 reducer 개수

		// set input and output directories
		FileInputFormat.addInputPath(job,new Path(otherArgs[0])); //입력 데이터가 있는 path
		FileOutputFormat.setOutputPath(job,new Path(otherArgs[1])); //결과를 출력할 path
		System.exit(job.waitForCompletion(true) ? 0 : 1 ); //실행
	}
}
```

**Driver.java 파일뿐만 아니라 Wordcount.java가 바뀌면 반드시 ant를 다시 수행해야함**
결과를 볼때는 항상 **outputDirectory를 삭제해야한다**
결과 파일들은 part-r-00000 .... 으로 나온다 (part-r-(reducer개수-1))

Main 함수에서 Mapper나 Reducer에 값을 Broadcast하려면

```
Configuration config = job.getConfiguration();
config.set("name","Shim"); //String 은 set , name이라는 symbol의 값은 shim이다.
config.setInt("one",1);
config.setFloat("point_five",(float)0.5);
```

이렇게 메인에서 지정하고

reducer나 mapper에서

```
protected void setup(Context context) throws IOExeption, InterruptedException { // setup은 mapper나 reduce에서 돌기전에 가장먼저 하는것
	Configuration config = context.getConfiguration();
	String name = config.get("name","kim"); // main에서 name이라는 symbol에 값이 broadcast되지 않았다면 default로 kim을 지정
	int point = config.getInt("one",1);
	float rate = config.getFloat("point_five",(float)0.5);
}
```
